{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "IN92KBEC1lGh"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist  #Used to load the mnist dataset\n",
    "import matplotlib.pyplot as plt   #Used to show graphs (All graphs are commented out)\n",
    "import numpy as np  \n",
    "import time                       #Used to calculate elapsed time\n",
    "np.set_printoptions(suppress=True)  #Disables scientific notation when printing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "VhKMwJV41s70"
   },
   "outputs": [],
   "source": [
    "#Data loading using keras mnist.load_data() function\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "#Reduces the number of dimensions from 3 to 2\n",
    "X_train=X_train.reshape(X_train.shape[0],X_train.shape[1]*X_train.shape[2])\n",
    "X_test = X_test.reshape(X_test.shape[0],X_test.shape[1]*X_test.shape[2])\n",
    "y_train = np.array(y_train, dtype=np.int8)\n",
    "y_test = np.array(y_test, dtype=np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "f87hrqcCmUvT"
   },
   "outputs": [],
   "source": [
    "#Encoding our y data to 2 classes , for even and odd numbers.\n",
    "def encode(A):\n",
    "  xE= []\n",
    "  for x in range(len(A)):\n",
    "    if (A[x]%2==0):\n",
    "      value = 1\n",
    "    else:\n",
    "      value=-1\n",
    "    xE.append(value)\n",
    "  return np.array(xE)\n",
    "\n",
    "#Normalizing our data by dividing them all by 255\n",
    "def normalize(X_array):\n",
    "  X_array=X_array/255\n",
    "  X_array=np.round(X_array,5)\n",
    "  return X_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "htXDp8O2LcFT"
   },
   "outputs": [],
   "source": [
    "#Applying the above functions to our data\n",
    "\n",
    "y_train = encode(y_train)\n",
    "y_test = encode(y_test)\n",
    "X_train = normalize(X_train)\n",
    "X_test = normalize(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "g_6o1s1Xz6Vs"
   },
   "outputs": [],
   "source": [
    "#Τaking smaller samples of \"size\" data to use\n",
    "index = np.random.choice(X_train.shape[0], size=10000, replace=False)\n",
    "index2 = np.random.choice(X_test.shape[0], size=1000, replace=False)\n",
    "X_train_sm=X_train[index]\n",
    "y_train_sm=y_train[index]\n",
    "X_test_sm=X_test[index2]\n",
    "y_test_sm=y_test[index2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "hyG5MhXwe-uT"
   },
   "outputs": [],
   "source": [
    "#Object for our svm. Initializes weights and bias to 0\n",
    "class Layer:\n",
    "  def __init__(self,X):\n",
    "    self.weights = np.zeros(len(X[0]))\n",
    "    self.b = 0   #Bias of each layer\n",
    "  \n",
    "\n",
    "\n",
    "#Function used to test our model after it's trained.\n",
    "def classify(svm,X,y):\n",
    "  correct = 0\n",
    "  incorrect = 0\n",
    "  for x in range(len(X)):\n",
    "    temp = np.sign(np.dot(X[x],svm.weights) - svm.b) #Tests with : xT*w - b\n",
    "    if (temp == y[x]):\n",
    "      correct+=1\n",
    "    else:\n",
    "      incorrect+=1\n",
    "\n",
    "  print(\"\\nNo of correct classifications : \",correct)\n",
    "  print(\"No of incorrect classifications : \",incorrect)\n",
    "  print(\"Accuracy percentage : \",correct*100/len(X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xUZxXNDxMGvj"
   },
   "outputs": [],
   "source": [
    "#Soft Margin without Quadratic Programming\n",
    "start = time.time()\n",
    "\n",
    "#Using smaller training sets to avoid overfitting\n",
    "X = X_train_sm\n",
    "y = y_train_sm\n",
    "svmT = Layer(X)\n",
    "\n",
    "#Learning rate and lamda hyperparameters\n",
    "lamda = 0.1 #Lamda parameter\n",
    "lr = 0.001  #Learning rate\n",
    "\n",
    "#Number of epochs for backpropagation. 1 is enough but can be increased for smaller learning rate.\n",
    "epochs = 1\n",
    "loss = np.zeros(epochs)\n",
    "#For each epoch -> for each sample of data, calculate output and update weights and bias based on if it's on the correct side.\n",
    "for e in range(epochs):\n",
    "  tempLoss = 0\n",
    "  for i in range(len(X)):\n",
    "    output = y[i] * (np.dot(X[i],svmT.weights) - svmT.b)\n",
    "\n",
    "    #Loss is increased only if sample is classified incorrectly\n",
    "    if (output < 1):\n",
    "      tempLoss += 1 - output\n",
    "      svmT.weights -= lr * (2 * lamda * svmT.weights - np.dot(X[i],y[i]))\n",
    "    else:\n",
    "      svmT.weights -= lr * (2 * lamda * svmT.weights)\n",
    "      svmT.b -= lr * y[i]\n",
    "\n",
    "  #Loss for each epoch. Can be plotted for epcohs>1    \n",
    "  loss[e] = tempLoss/784 - lamda *np.dot(svmT.weights,svmT.weights)\n",
    "\n",
    "#Classify test data \n",
    "X = X_test #X_test_sm + y_test_sm can be used for smaller test set\n",
    "y = y_test \n",
    "classify(svmT,X,y)\n",
    "\n",
    "end = time.time()\n",
    "print (\"\\nTime to train and test model: \",float(\"{:.2f}\".format((end - start)/60)),\" minutes\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Loss: \",loss[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "0cNZLWbkqdz5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.9448e+03 -2.4075e+03  1e+05  3e+01  1e-12\n",
      " 1: -3.5447e+02 -2.2639e+03  8e+03  2e+00  1e-12\n",
      " 2: -2.4282e+02 -1.4231e+03  2e+03  4e-01  4e-13\n",
      " 3: -2.1203e+02 -7.8746e+02  9e+02  2e-01  2e-13\n",
      " 4: -2.0507e+02 -3.8032e+02  2e+02  4e-02  2e-13\n",
      " 5: -2.1807e+02 -2.8381e+02  8e+01  1e-02  2e-13\n",
      " 6: -2.2377e+02 -2.6283e+02  4e+01  5e-03  2e-13\n",
      " 7: -2.2754e+02 -2.5141e+02  3e+01  2e-03  2e-13\n",
      " 8: -2.3031e+02 -2.4161e+02  1e+01  2e-15  3e-13\n",
      " 9: -2.3229e+02 -2.3885e+02  7e+00  7e-15  2e-13\n",
      "10: -2.3395e+02 -2.3652e+02  3e+00  2e-15  3e-13\n",
      "11: -2.3472e+02 -2.3556e+02  8e-01  7e-16  3e-13\n",
      "12: -2.3500e+02 -2.3520e+02  2e-01  5e-15  3e-13\n",
      "13: -2.3508e+02 -2.3511e+02  2e-02  9e-16  3e-13\n",
      "14: -2.3510e+02 -2.3510e+02  2e-03  2e-15  3e-13\n",
      "15: -2.3510e+02 -2.3510e+02  7e-05  7e-15  3e-13\n",
      "Optimal solution found.\n",
      "\n",
      "No of correct classifications :  8966\n",
      "No of incorrect classifications :  1034\n",
      "Accuracy percentage :  89.66\n",
      "\n",
      "No of correct classifications :  53830\n",
      "No of incorrect classifications :  6170\n",
      "Accuracy percentage :  89.71666666666667\n",
      "\n",
      "Time to train and test model:  4.51  minutes\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "#Soft margin with Quadratic Programming and no kernel\n",
    "X = X_train_sm\n",
    "y = y_train_sm\n",
    "svmQuad = Layer(X)\n",
    "\n",
    "#Hyperparameter C\n",
    "C = 0.1 \n",
    "\n",
    "#Using quadratic solver CVXOPT\n",
    "#Converting our problem to one solvable by our quad solver\n",
    "#Calculating matrices for our quadratic solver so that our problem is:\n",
    "#Minimize 1/2xT Px + qTx #subject to Gx<=h and Ax=b\n",
    "\n",
    "\n",
    "Q = np.dot(X,X.T)\n",
    "Q2 = np.outer(y,y)\n",
    "\n",
    "#Matrix P = Σyk*yl(xk.xl)\n",
    "P = Q*Q2\n",
    "\n",
    "q =  - np.ones((len(X),1))\n",
    "b = np.zeros(1)\n",
    "#A matrix is calculated directly as a cvxopt matrix\n",
    "G_A = -np.eye(len(X)) \n",
    "G = np.vstack((G_A, np.identity(len(X))))\n",
    "h = np.hstack((np.zeros(len(X)), np.ones(len(X)) * C))\n",
    "\n",
    "#Converting our numpy arrays to cvxopt matrices\n",
    "from cvxopt import matrix \n",
    "from cvxopt import solvers \n",
    "P = matrix(P)\n",
    "q = matrix(q)\n",
    "G = matrix(G)\n",
    "h = matrix(h)\n",
    "A = matrix(y, (1,len(X)),'d')\n",
    "b = matrix(b)\n",
    "\n",
    "quadSolution = solvers.qp(P, q, G, h, A, b)\n",
    "\n",
    "#Taking the alphas from the solved function\n",
    "alphas = np.array(quadSolution['x'])\n",
    "\n",
    "#Taking the indexes of the alphas where a>0 (a>0.00001)\n",
    "index = []\n",
    "alphasBool = (alphas > 1e-5)\n",
    "for i in range(len(alphasBool)):\n",
    "  if alphasBool[i]==True:\n",
    "    index.append(i) \n",
    "\n",
    "#Taking only the samples where a>0\n",
    "aSV = alphas[index]\n",
    "XSV = X[index]\n",
    "ySV = y[index]\n",
    "ySV = ySV.reshape(-1,1)\n",
    "\n",
    "#Calculating weights and bias\n",
    "svmQuad.weights =np.dot(XSV.T,ySV*aSV)\n",
    "\n",
    "#Bias is taken as a mean from every support vector\n",
    "for i in range(len(XSV)):                           \n",
    "  svmQuad.b += np.dot(svmQuad.weights.T,XSV[i]) - ySV[i]\n",
    "svmQuad.b /= len(XSV)\n",
    "\n",
    "#Classifying test data\n",
    "X = X_test\n",
    "y = y_test\n",
    "classify(svmQuad,X,y)\n",
    "\n",
    "X = X_train\n",
    "y = y_train\n",
    "classify(svmQuad,X,y)\n",
    "end = time.time()\n",
    "print (\"\\nTime to train and test model: \",float(\"{:.2f}\".format((end - start)/60)),\" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "zFplKe9omRjl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train dataset:  94.62 %\n",
      "Accuracy on test dataset:  95.78999999999999 %\n",
      "\n",
      "Time to train and test model:  0.29  minutes\n"
     ]
    }
   ],
   "source": [
    "#Using sklearn to create Polynomial and RBF Kernel SVMs to compare with no kernel soft margin\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "start = time.time()\n",
    "\n",
    "#SVM With poly kernel and degree 3\n",
    "clf = SVC(C=100,kernel='rbf',gamma=0.01)\n",
    "clf.fit(X_test_sm,y_test_sm)\n",
    "\n",
    "accuracyTrain = accuracy_score(y_train,clf.predict(X_train))\n",
    "accuracyTest = accuracy_score(y_test,clf.predict(X_test))\n",
    "\n",
    "print(\"Accuracy on train dataset: \",accuracyTrain*100,\"%\")\n",
    "print(\"Accuracy on test dataset: \",accuracyTest*100,\"%\")\n",
    "\n",
    "end = time.time()\n",
    "print (\"\\nTime to train and test model: \",float(\"{:.2f}\".format((end - start)/60)),\" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VRIOvh16vA0u"
   },
   "outputs": [],
   "source": [
    "#Using GridSearchCV to apply multiple parameter combinations and find the best\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "param_grid = [\n",
    "            {\"C\": [0.1, 1, 10,100],\"degree\":[2,3,4] ,\"kernel\" : [\"poly\"]},\n",
    "            {\"C\": [0.1, 1, 10, 100], \"gamma\": [0.0001,0.001,0.1,1], \"kernel\": [\"rbf\"]}]\n",
    "\n",
    "clf = GridSearchCV(SVC(),param_grid)\n",
    "clf.fit(X_train_sm, y_train_sm)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "O_Yrtjd1d5Ok"
   },
   "outputs": [],
   "source": [
    "#Data regarding the Parameter grid results from GridSearchCV  - - clf.best_params_ returns the best parameter combination from our grid\n",
    "scores = clf.cv_results_\n",
    "print(scores)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L6Q3vPMHewUn"
   },
   "outputs": [],
   "source": [
    "#SVM With poly kernel and degree 3\n",
    "clf = SVC(C=10,kernel='poly',degree=3)\n",
    "clf.fit(X_test_sm,y_test_sm)\n",
    "\n",
    "accuracyTrain = accuracy_score(y_train,clf.predict(X_train))\n",
    "accuracyTest = accuracy_score(y_test,clf.predict(X_test))\n",
    "\n",
    "print(\"Accuracy on train dataset: \",accuracyTrain*100,\"%\")\n",
    "print(\"Accuracy on test dataset: \",accuracyTest*100,\"%\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NN_SVM_Colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
